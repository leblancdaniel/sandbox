import os
import pandas as pd 
import spacy
from spacy.util import minibatch
import random 

# Load data
def load_data(filepath, split=0.9):
    """ Returns 90/10 train-test split of .csv columns "texts" for X and reviews for labels 
        
        Arguments
        ---------
        filepath: filepath to .csv file containing yelp reviews with texts of reviews and review scores 1-5

    """
    data = pd.read_csv(filepath)
    # Shuffle data
    train_data = data.sample(frac=1, random_state=7)
    texts = train_data.text.values
    labels = [{"POSITIVE": bool(y), "NEGATIVE": not bool(y)}
              for y in train_data.sentiment.values]
    split = int(len(train_data) * split)
    train_labels = [{"cats": labels} for labels in labels[:split]]
    val_labels = [{"cats": labels} for labels in labels[split:]]
    
    return texts[:split], train_labels, texts[split:], val_labels

# Train function
def train(model, train_data, optimizer):
    """ Returns losses generated by each train session on every batch 
    
        Arguments
        ---------
        model: SpaCy model with a TextCategorizer
        train_data: list of train_texts and train_labels from load_data function
        optimizer: sgd = nlp.begin_training()
    
    """
    losses = {}
    batch_size = 8
    random.seed(1)
    random.shuffle(train_data)
    batches = minibatch(train_data, size=batch_size)
    for batch in batches:
        # train_data is a list of tuples [(text0, label0), (text1, label1), ...]
        # Split batch into texts and labels
        texts, labels = zip(*batch)
        # Update model with texts and labels
        model.update(texts, labels, sgd=optimizer, losses=losses)
        
    return losses

# Make predictions
def predict(model, texts): 
    """ Returns the predicted class of a TextCategorizer model.

        Arguments
        ---------
        model: SpaCy model with a Text Categorizer
        texts: Text samples, from load_data function
    
    """
    # Use the model's tokenizer to tokenize each input text
    docs = [model.tokenizer(text) for text in texts]
    textcat = model.get_pipe('textcat')
    # Use textcat to get the scores for each doc
    scores, _ = textcat.predict(docs)
    # From the scores, find the class with the highest score/probability
    predicted_class = scores.argmax(axis=1)
    
    return predicted_class

# Evaluate model
def evaluate(model, texts, labels):
    """ Returns the accuracy of a TextCategorizer model. 
    
        Arguments
        ---------
        model: SpaCy model with a TextCategorizer
        texts: Text samples, from load_data function
        labels: True labels, from load_data function
    
    """
    # Get predictions from textcat model (using your predict method)
    predicted_class = predict(model, texts)
    # From labels, get the true class as a list of integers (POSITIVE -> 1, NEGATIVE -> 0)
    true_class = [int(each['cats']['POSITIVE']) for each in labels]
    # A boolean or int array indicating correct predictions
    correct_predictions = predicted_class == true_class
    # The accuracy, number of correct predictions divided by all predictions
    accuracy = correct_predictions.mean()
    
    return accuracy

# filepath to yelp_ratings.csv
script_dir = os.path.dirname(__file__)
rel_path = "yelp_ratings.csv"
filepath = os.path.join(script_dir, rel_path)
# Load data
train_texts, train_labels, val_texts, val_labels = load_data(filepath)
# Create an empty model
nlp = spacy.blank("en")
# Create the TextCategorizer with exclusive classes and "bow" architecture
textcat = nlp.create_pipe(
              "textcat",
              config={
                "exclusive_classes": True,
                "architecture": "bow"})
# Add the TextCategorizer to the empty model
nlp.add_pipe(textcat)
# Add labels to text classifier
textcat.add_label("NEGATIVE")
textcat.add_label("POSITIVE")
# Fix seed for reproducibility
spacy.util.fix_random_seed(1)
random.seed(1)
# Define optimizer and train_data
optimizer = nlp.begin_training()
train_data = list(zip(train_texts, train_labels))
# Train, predict, and evaluate model over {n_iters} epochs
n_iters = 10
for i in range(n_iters):
    losses = train(nlp, train_data, optimizer)
    accuracy = evaluate(nlp, val_texts, val_labels)
    print(f"Loss: {losses['textcat']:.3f} \t Accuracy: {accuracy:.3f}")
